"""
Twitter/X Scraper using Selenium
Uses existing Chrome browser + Cookies
Reliable & Anti-Ban Friendly

Built by Friday for skripsi project
"""
import time
import json
import csv
import argparse
import logging
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# Import config for consistent settings
try:
    from config import SCRAPER, WORKERS
    SCRAPER_CONFIG = SCRAPER
except ImportError:
    # Fallback if run standalone
    SCRAPER_CONFIG = {
        'max_scroll_attempts': 100,
        'coffee_break_interval': 50,
        'coffee_break_duration': 15,
        'scroll_delay_min': 1.5,
        'scroll_delay_max': 4.0
    }

# User Cookies (Loaded from file)
def load_cookies():
    try:
        with open('cookies_config.json', 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        print("âš ï¸ 'cookies_config.json' not found. Using dummy cookies.")
        return {}

COOKIES_DICT = load_cookies()

def setup_driver(headless=False):
    options = Options()
    if headless:
        options.add_argument('--headless=new')  # New headless mode (faster)
        
        # === HEADLESS OPTIMIZATIONS ===
        # Disable images (saves bandwidth and memory)
        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.default_content_setting_values.notifications": 2,
            "profile.managed_default_content_settings.stylesheets": 2,
            "profile.managed_default_content_settings.fonts": 2,
        }
        options.add_experimental_option("prefs", prefs)
        
        # Additional memory optimizations
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-extensions')
        options.add_argument('--disable-infobars')
        options.add_argument('--memory-pressure-off')
        options.add_argument('--single-process')
        
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('--log-level=3')
    
    # Anti-detection
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    options.add_argument('--disable-software-rasterizer') # Fix for renderer timeouts
    options.add_argument('--disable-dev-shm-usage') # Ensure this is present
    
    # PERF: Eager strategy (interact as soon as DOM is ready, don't wait for all assets)
    options.page_load_strategy = 'eager'
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    
    # Set timeouts to prevent infinite hanging (Increased to 120s for slow connections)
    driver.set_page_load_timeout(120)
    driver.set_script_timeout(120)
    
    return driver

def check_account_health():
    """
    Check if the Twitter account is healthy (not shadowbanned or restricted).
    Returns a dict with health status and details.
    """
    print("ðŸ©º Checking account health...")
    driver = setup_driver(headless=True)
    
    result = {
        "status": "UNKNOWN",
        "can_search": False,
        "can_see_tweets": False,
        "warnings": [],
        "recommendation": ""
    }
    
    try:
        # 1. Navigate to Twitter
        driver.get("https://x.com")
        time.sleep(3)
        
        # 2. Inject cookies
        if COOKIES_DICT:
            for cookie_name, cookie_value in COOKIES_DICT.items():
                if cookie_value:
                    try:
                        driver.add_cookie({
                            "name": cookie_name,
                            "value": cookie_value,
                            "domain": ".x.com"
                        })
                    except Exception as e:
                        logging.debug(f"Cookie injection warning for {cookie_name}: {e}")
            driver.refresh()
            time.sleep(3)
        
        # 3. Check if logged in (look for home timeline or compose button)
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "a[data-testid='SideNav_NewTweet_Button'], div[data-testid='tweetButtonInline']"))
            )
            print("   âœ… Login successful")
        except Exception as e:
            logging.debug(f"Login check timeout: {e}")
            result["status"] = "ERROR"
            result["warnings"].append("Login failed - cookies may be expired")
            result["recommendation"] = "Please update your cookies in cookies_config.json"
            return result
        
        # 4. Try searching for a common term
        driver.get("https://x.com/search?q=test&src=typed_query&f=live")
        time.sleep(4)
        
        # 5. Check if search results appear
        try:
            articles = driver.find_elements(By.CSS_SELECTOR, "article[data-testid='tweet']")
            if len(articles) > 0:
                result["can_search"] = True
                result["can_see_tweets"] = True
                print(f"   âœ… Search works - found {len(articles)} tweets")
            else:
                # Check for "No results" message
                no_results = driver.find_elements(By.XPATH, "//*[contains(text(), 'No results')]")
                if no_results:
                    result["warnings"].append("Search returned no results - possible shadowban")
                else:
                    result["warnings"].append("Could not load tweets - possible rate limit")
        except Exception as e:
            result["warnings"].append(f"Search test failed: {str(e)}")
        
        # 6. Determine overall status
        if result["can_search"] and result["can_see_tweets"]:
            result["status"] = "HEALTHY"
            result["recommendation"] = "Account is healthy. Safe to scrape."
        elif result["warnings"]:
            result["status"] = "WARNING"
            result["recommendation"] = "Account may have issues. Consider waiting 24h or using backup account."
        else:
            result["status"] = "UNKNOWN"
            result["recommendation"] = "Could not determine status. Proceed with caution."
            
    except Exception as e:
        result["status"] = "ERROR"
        result["warnings"].append(str(e))
        result["recommendation"] = "Health check failed. Check your internet connection."
    finally:
        driver.quit()
    
    print(f"ðŸ©º Health Status: {result['status']}")
    return result

# ... (previous imports)
import random
import os
import re

# ... (cookies dict)

def random_delay(min_sec=1.5, max_sec=4.0):
    """Sleep for a random amount of time to simulate human behavior"""
    time.sleep(random.uniform(min_sec, max_sec))

import html
import unicodedata

def normalize_unicode_fonts(text):
    """
    Convert fancy Unicode fonts (Mathematical Bold, Script, etc.) to regular ASCII.
    These are commonly used in Twitter for stylized text like ð—•ð—¼ð—¹ð—± ð—§ð—²ð˜…ð˜.
    """
    if not text:
        return text
    
    # Unicode block mappings for common styled fonts
    # Mathematical Bold (ð—”-ð—­, ð—®-ð˜‡, ðŸ¬-ðŸµ)
    # Mathematical Sans-Serif Bold (ð—”-ð—­, ð—®-ð˜‡)
    result = []
    for char in text:
        code = ord(char)
        
        # Mathematical Bold Caps (ð€-ð™) -> A-Z
        if 0x1D400 <= code <= 0x1D419:
            result.append(chr(code - 0x1D400 + ord('A')))
        # Mathematical Bold Small (ðš-ð³) -> a-z
        elif 0x1D41A <= code <= 0x1D433:
            result.append(chr(code - 0x1D41A + ord('a')))
        # Mathematical Sans-Serif Bold Caps (ð—”-ð—­) -> A-Z
        elif 0x1D5D4 <= code <= 0x1D5ED:
            result.append(chr(code - 0x1D5D4 + ord('A')))
        # Mathematical Sans-Serif Bold Small (ð—®-ð˜‡) -> a-z
        elif 0x1D5EE <= code <= 0x1D607:
            result.append(chr(code - 0x1D5EE + ord('a')))
        # Mathematical Bold Digits (ðŸŽ-ðŸ—) -> 0-9
        elif 0x1D7CE <= code <= 0x1D7D7:
            result.append(chr(code - 0x1D7CE + ord('0')))
        # Mathematical Sans-Serif Bold Digits (ðŸ¬-ðŸµ) -> 0-9  
        elif 0x1D7EC <= code <= 0x1D7F5:
            result.append(chr(code - 0x1D7EC + ord('0')))
        # Fullwidth Latin (ï¼¡-ï¼º, ï½-ï½š) -> A-Z, a-z
        elif 0xFF21 <= code <= 0xFF3A:
            result.append(chr(code - 0xFF21 + ord('A')))
        elif 0xFF41 <= code <= 0xFF5A:
            result.append(chr(code - 0xFF41 + ord('a')))
        else:
            result.append(char)
    
    return ''.join(result)

def is_indonesian_text(text):
    """
    Check if text is likely Indonesian/Latin-based.
    Returns False for Korean, Chinese, Japanese, Arabic, etc.
    """
    if not text:
        return False
    
    # Count Latin characters vs non-Latin
    latin_count = 0
    non_latin_count = 0
    
    for char in text:
        code = ord(char)
        # Basic Latin, Latin Extended, Latin Supplement
        if (0x0000 <= code <= 0x024F) or char.isspace() or char.isdigit():
            latin_count += 1
        # Korean (Hangul)
        elif 0xAC00 <= code <= 0xD7AF or 0x1100 <= code <= 0x11FF:
            non_latin_count += 1
        # Chinese (CJK)
        elif 0x4E00 <= code <= 0x9FFF:
            non_latin_count += 1
        # Japanese (Hiragana, Katakana)
        elif 0x3040 <= code <= 0x30FF:
            non_latin_count += 1
        # Arabic
        elif 0x0600 <= code <= 0x06FF:
            non_latin_count += 1
    
    # If more than 20% non-Latin, likely not Indonesian
    total = latin_count + non_latin_count
    if total == 0:
        return True
    
    return (non_latin_count / total) < 0.2

def is_quality_text(text):
    """
    Check if text is quality content (not spam, ASCII art, or meaningless).
    Returns False for:
    - Very short text (< 20 chars)
    - Mostly whitespace or special chars
    - ASCII art patterns
    - Repeated characters (like "JAJAJAJA")
    """
    if not text:
        return False
    
    # Remove URLs and mentions for analysis
    clean = re.sub(r'http\S+', '', text)
    clean = re.sub(r'@\w+', '', clean)
    clean = clean.strip()
    
    # Too short after cleaning
    if len(clean) < 15:
        return False
    
    # Count meaningful characters (letters and Indonesian words)
    letters = sum(1 for c in clean if c.isalpha())
    total = len(clean)
    
    # Less than 40% letters = probably spam/art
    if total > 0 and (letters / total) < 0.4:
        return False
    
    # Check for repeated character patterns (like "JAJAJA" or "HAHAHA")
    if re.search(r'(.)\1{4,}', clean):  # Same char 5+ times
        return False
    if re.search(r'(..)\1{3,}', clean):  # Same 2-char pattern 4+ times (JAJA repeated)
        return False
    
    # Check for excessive newlines (ASCII art pattern)
    if clean.count('\n') > 5:
        return False
    
    return True

def clean_text(text):
    """Start-of-the-art preprocessing with Unicode normalization"""
    if not text: return ""
    
    # 0. Normalize Unicode fancy fonts (ð—•ð—¼ð—¹ð—± -> Bold)
    text = normalize_unicode_fonts(text)
    
    # 1. Decode HTML entities (&amp; -> &)
    text = html.unescape(text)
    
    # 2. Remove URLs
    text = re.sub(r'http\S+', '', text)
    
    # 3. Remove Mentions (@user)
    text = re.sub(r'@\w+', '', text)
    
    # 4. Remove Emojis & Special Symbols (Keep only alphanumeric, punctuation, and basic latin)
    # This regex keeps letters, numbers, spaces, and basic punctuation
    text = re.sub(r'[^\w\s,.:;!?#"\'-]', '', text)
    
    # 5. Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text.lower() # Optional: Lowercase

def parse_metric(element):
    """Extract number from aria-label or text"""
    try:
        val = element.get_attribute("aria-label").split()[0] # "15 replies" -> "15"
        val = val.replace(',', '').replace('.', '')
        if 'K' in val: return int(float(val.replace('K', '')) * 1000)
        if 'M' in val: return int(float(val.replace('M', '')) * 1000000)
        return int(val)
    except Exception:
        return 0  # Silent fail for missing metrics is acceptable

def save_intermediate(tweets, filename):
    """Save progress incrementally (JSON + CSV)"""
    # JSON
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(tweets, f, ensure_ascii=False, indent=2)
        
    # CSV
    csv_filename = filename.replace('.json', '.csv')
    if tweets:
        keys = tweets[0].keys()
        with open(csv_filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=keys)
            writer.writeheader()
            writer.writerows(tweets)

def scrape_twitter(keyword, count=20, headless=False, output_filename=None, progress_callback=None, filter_keywords=None, is_cancelled_func=None):
    def log(msg):
        logging.info(msg)      # Log to file AND console (via handlers)
        if progress_callback:
            progress_callback(msg)

    log(f"ðŸ¦ Starting Selenium Scraper (Safe Mode)")
    log(f"   Keyword: {keyword}")
    if filter_keywords:
        log(f"   Filtering Strictness: ON (Must contain: {filter_keywords})")
    log(f"   Target: {count} tweets")
    
    driver = setup_driver(headless)
    tweets = []
    seen_texts_hashes = set()  # Track seen tweets in this session to avoid re-processing

    # Determine filename
    if output_filename:
        filename = output_filename
    else:
        # Clean keyword for filename and TRUNCATE to avoid Windows 260 char path limit
        clean_kw = "".join([c if c.isalnum() else "_" for c in keyword])
        clean_kw = clean_kw[:100]  # Truncate to 100 chars max
        # Save to outputs folder by default
        os.makedirs("outputs", exist_ok=True)
        filename = f"outputs/tweets_{clean_kw}.json"

    try:
        # 1. Login/Cookie Injection
        log("ðŸŒ Navigating to x.com...")
        
        # RETRY LOGIC for Navigation (Fixes Renderer Timeouts)
        for attempt in range(3):
            try:
                driver.get("https://x.com/404")
                break
            except Exception as e:
                log(f"   âš ï¸ Navigation timeout (Attempt {attempt+1}/3). Retrying...")
                if attempt == 2: raise e
                time.sleep(5)
                
        random_delay(2, 3)
        
        log("ðŸª Injecting cookies...")
        for name, value in COOKIES_DICT.items():
            driver.add_cookie({'name': name, 'value': value, 'domain': '.x.com', 'path': '/'})
            try:
                driver.add_cookie({'name': name, 'value': value, 'domain': '.twitter.com', 'path': '/'})
            except Exception:
                pass  # Twitter.com domain may not accept all cookies

        # 3. Search
        log("ðŸ” Going to search page...")
        search_url = f"https://x.com/search?q={keyword}&src=typed_query&f=live"
        
        # RETRY LOGIC for Search Navigation
        for attempt in range(3):
            try:
                driver.get(search_url)
                break
            except Exception as e:
                log(f"   âš ï¸ Search navigation timeout (Attempt {attempt+1}/3). Retrying...")
                if attempt == 2: raise e
                time.sleep(5)
                
        random_delay(3, 5)
        
        # 4. Wait for content and Validate Page
        try:
            # Wait for either tweets or "No results" or redirect
            WebDriverWait(driver, 15).until(
                lambda d: "search" in d.current_url or "home" in d.current_url or "login" in d.current_url
            )
            
            current_url = driver.current_url.lower()
            log(f"ðŸ”— Current URL: {current_url}")
            
            if "login" in current_url or "i/flow/login" in current_url:
                log(f"âš ï¸ Redirected to Login page. Auth failed.")
                return []
                
            if "home" in current_url and "search" not in current_url:
                log(f"âš ï¸ Redirected to Home (For You) page. Search failed (Account likely restricted or cookies invalid).")
                return []
                
            # Verify we are on search page
            try:
                WebDriverWait(driver, 20).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "article[data-testid='tweet']"))
                )
            except:
                if "No results for" in driver.page_source:
                    log("â„¹ï¸ No results found for this query.")
                    return []
                log("âš ï¸ Timeout waiting for tweets. Possible network issue or strict rate limit.")
                return []
        except Exception as e:
            log(f"âš ï¸ Page load error: {e}")
            # If page load fails badly (timeout), try one refresh before giving up
            try:
                driver.refresh()
                time.sleep(5)
            except:
                pass
            return []
        
        # 5. Scrape loop
        log("ðŸ“œ Scrolling and collecting...")
        last_height = driver.execute_script("return document.body.scrollHeight")
        scroll_attempts = 0
        consecutive_no_new_tweets = 0
        max_scroll_attempts = SCRAPER_CONFIG.get('max_scroll_attempts', 100)
        
        # ===== HUMAN SIMULATION MODE =====
        # Random Activity Pattern: Variable speed to look natural
        # Intelligent Pacing: Start slow, speed up gradually, slow down on rate limit
        session_start_time = time.time()
        tweets_this_session = 0
        current_pace = "slow"  # slow -> medium -> fast (adapts over time)
        rate_limit_warnings = 0
        
        def get_human_delay():
            """Return delay based on current pace - simulates human behavior"""
            nonlocal current_pace, rate_limit_warnings
            
            # Random variation factor (0.7 to 1.5x)
            variation = random.uniform(0.7, 1.5)
            
            # Occasionally take a longer "distraction" pause (5% chance)
            if random.random() < 0.05:
                distraction_time = random.uniform(5, 15)
                logging.info(f"   ðŸ§‘ Human pause ({distraction_time:.1f}s)...")
                return distraction_time
            
            # Base delays by pace
            if current_pace == "slow":
                base = random.uniform(3.0, 5.0)
            elif current_pace == "medium":
                base = random.uniform(2.0, 4.0)
            else:  # fast
                base = random.uniform(1.5, 3.0)
            
            # If rate limited recently, slow down
            if rate_limit_warnings > 0:
                base *= (1 + rate_limit_warnings * 0.5)
            
            return base * variation
        
        def update_pace():
            """Intelligent pacing: adjust speed based on session progress"""
            nonlocal current_pace, tweets_this_session
            
            elapsed_minutes = (time.time() - session_start_time) / 60
            
            # Start slow, gradually speed up (like a human warming up)
            if elapsed_minutes < 2:
                current_pace = "slow"
            elif elapsed_minutes < 10:
                current_pace = "medium"
            else:
                current_pace = "fast"
            
            # But slow down if we've collected a lot quickly
            if tweets_this_session > 100 and elapsed_minutes < 5:
                current_pace = "slow"  # Too fast, slow down!
                logging.info("   âš ï¸ Slowing down pace to avoid detection...")
        
        while len(tweets) < count and scroll_attempts < max_scroll_attempts:
            # CHECK CANCELLATION
            if is_cancelled_func and is_cancelled_func():
                log("ðŸ›‘ Job cancelled by user. Stopping scraper...")
                break
                
            # Get articles
            articles = driver.find_elements(By.CSS_SELECTOR, "article[data-testid='tweet']")
            new_tweets_found = False
            items_processed_this_loop = 0 # Track if we saw NEW content (even if filtered)
            
            for article in articles:
                if len(tweets) >= count:
                    break
                try:
                    # Parse Tweet
                    text_el = article.find_element(By.CSS_SELECTOR, "div[data-testid='tweetText']")
                    original_text = text_el.text
                    
                    # Create simple hash for uniqueness check within session
                    # This prevents re-logging "Filtered" for the same tweet over and over
                    text_hash = hash(original_text[:50] + str(len(original_text)))
                    if text_hash in seen_texts_hashes:
                        continue
                    seen_texts_hashes.add(text_hash)
                    items_processed_this_loop += 1
                    
                    user_el = article.find_element(By.CSS_SELECTOR, "div[data-testid='User-Name'] a")
                    username = user_el.get_attribute("href").split('/')[-1]
                    
                    # Filter out Grok AI
                    if username.strip().lower() == 'grok':
                        continue
                    
                    # Time & URL
                    time_el = article.find_element(By.TAG_NAME, "time")
                    timestamp = time_el.get_attribute("datetime")
                    tweet_url = user_el.get_attribute("href") # Actually the link is on the time element usually, but username link is to profile. 
                    # Correct URL logic: usually the timestamp IS the permalink
                    try:
                        tweet_url = time_el.find_element(By.XPATH, "./..").get_attribute("href")
                    except:
                        tweet_url = f"https://x.com/{username}"

                    # Metrics
                    replies = 0
                    retweets = 0
                    likes = 0
                    views = 0
                    
                    # Try multiple methods to extract engagement metrics
                    try:
                        # Method 1: Using button's aria-label
                        reply_btn = article.find_element(By.CSS_SELECTOR, "button[data-testid='reply']")
                        replies = parse_metric(reply_btn)
                    except: pass
                    
                    try:
                        retweet_btn = article.find_element(By.CSS_SELECTOR, "button[data-testid='retweet']")
                        retweets = parse_metric(retweet_btn)
                    except: pass
                    
                    try:
                        like_btn = article.find_element(By.CSS_SELECTOR, "button[data-testid='like']")
                        likes = parse_metric(like_btn)
                    except: pass
                    
                    # Method 2: Try the group container if buttons failed
                    if replies == 0 and retweets == 0 and likes == 0:
                        try:
                            group = article.find_element(By.CSS_SELECTOR, "div[role='group']")
                            buttons = group.find_elements(By.CSS_SELECTOR, "button")
                            if len(buttons) >= 4:
                                replies = parse_metric(buttons[0])
                                retweets = parse_metric(buttons[1])
                                likes = parse_metric(buttons[2])
                        except: pass
                    
                    # Filter out non-Indonesian text (Korean, Chinese, Japanese, Arabic)
                    if not is_indonesian_text(original_text):
                        continue  # Skip this tweet
                    
                    # Filter out spam, ASCII art, and low-quality content
                    if not is_quality_text(original_text):
                        continue  # Skip this tweet
                        
                    # STRICT KEYWORD FILTERING
                    if filter_keywords:
                        text_lower = original_text.lower()
                        # Check if any of the filter keywords are in the text
                        matched = False
                        for k in filter_keywords:
                            # Simple substring match (case insensitive)
                            if k.lower() in text_lower:
                                matched = True
                                break
                        
                        if not matched:
                            log(f"   ðŸ—‘ï¸ Filtered: {original_text[:30]}... (No keyword match)")
                            continue
                    
                    tweets.append({
                        "username": username,
                        "text": original_text,
                        "text_clean": clean_text(original_text),
                        "hashtags": re.findall(r'#\w+', original_text),
                        "timestamp": timestamp,
                        "url": tweet_url,
                        "replies": replies,
                        "retweets": retweets,
                        "likes": likes,
                        "scraped_at": datetime.now().isoformat(),
                    })
                    new_tweets_found = True
                    
                    # Log progress
                    if len(tweets) % 10 == 0:
                        log(f"   Collected {len(tweets)}/{count} tweets...")
                        save_intermediate(tweets, filename)
                        
                    # Track tweets for pacing
                    tweets_this_session += 1
                    update_pace()  # Adjust pace based on progress
                    
                    # LONG BREAK every N tweets (configurable to avoid shadowban)
                    coffee_interval = SCRAPER_CONFIG.get('coffee_break_interval', 50)
                    coffee_duration = SCRAPER_CONFIG.get('coffee_break_duration', 15)
                    if len(tweets) % coffee_interval == 0:
                        log(f"   â˜• Taking a coffee break ({coffee_duration}s) to avoid rate limiting...")
                        time.sleep(coffee_duration)
                        
                except:
                    continue
            
            # Scroll logic
            # If we found new tweets OR we processed new items (even if filtered), we are making progress
            if new_tweets_found or items_processed_this_loop > 0:
                consecutive_no_new_tweets = 0
                scroll_attempts = 0
            else:
                consecutive_no_new_tweets += 1
            
            # Scroll down with human-like randomness
            scroll_px = random.randint(600, 1000) # Smaller, more natural scroll
            driver.execute_script(f"window.scrollBy(0, {scroll_px});")
            
            # Use intelligent human delay:
            # If we found something good: act like a human reading
            # If we just filtered trash: fast scroll (scan mode)
            if new_tweets_found:
                human_wait = get_human_delay()
            else:
                # FAST MODE: If filtering heavily, don't wait 5s to look at trash
                human_wait = random.uniform(1.0, 2.0)
            
            time.sleep(human_wait)
            
            # Check if stuck
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                scroll_attempts += 1
                consecutive_no_new_tweets += 1
                
                # Try harder: Super scroll + wait
                if scroll_attempts % 5 == 0:
                    log(f"   ðŸ”„ Retrying scroll (Attempt {scroll_attempts}/{max_scroll_attempts})...")
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(3)
                    rate_limit_warnings += 1  # Track as potential rate limit
                
                # AGGRESSIVE RETRY: Scroll to top and back every 10 failed attempts
                if scroll_attempts % 10 == 0 and scroll_attempts < max_scroll_attempts:
                    log(f"   ðŸ”„ Aggressive scroll reset...")
                    driver.execute_script("window.scrollTo(0, 0);")
                    time.sleep(2)
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(3)
                
                # SMART RETRY: Refresh page every 15 failed attempts (was 20)
                if consecutive_no_new_tweets >= 8 and scroll_attempts % 15 == 0:
                    log(f"   ðŸ”ƒ Page refresh to get more data...")
                    driver.refresh()
                    time.sleep(5)
                    try:
                        WebDriverWait(driver, 10).until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, "article[data-testid='tweet']"))
                        )
                    except:
                        pass
                    consecutive_no_new_tweets = 0
            else:
                last_height = new_height
                consecutive_no_new_tweets = 0
                scroll_attempts = max(0, scroll_attempts - 1) # Decay if making progress
        
        # Final status
        if len(tweets) < count:
            log(f"âš ï¸ Only found {len(tweets)} tweets (Target: {count}). Twitter might not have more data for this query.")
                
        print(f"\nâœ… Successfully scraped {len(tweets)} tweets!")
        return tweets
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        logging.error(f"Scraper Error: {e}")
        return tweets
        
    finally:
        if tweets:
            save_intermediate(tweets, filename)
            print(f"ðŸ“ Saved to {filename}")
        driver.quit()


def export_json(tweets, filename):
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(tweets, f, ensure_ascii=False, indent=2)
    print(f"ðŸ“ Exported to {filename}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Selenium Twitter Scraper")
    parser.add_argument('-k', '--keyword', required=True)
    parser.add_argument('-c', '--count', type=int, default=20)
    parser.add_argument('-o', '--output', help="Custom output filename")
    parser.add_argument('--headless', action='store_true')
    
    args = parser.parse_args()
    
    # Allow custom output filename
    if args.output:
        # Override the logic inside (requires small change or just pass it differently)
        # Actually easier to just modify the filename logic inside scrape_twitter or pass it here
        # Let's modify scrape_twitter to accept filename
        pass 

    # For parallel scraping, the 'keyword' might actually be a full query string like "jokowi since:2024-01-01"
    # The existing script puts it into f"https://x.com/search?q={keyword}..." which works fine!
    
    final_filename = args.output
    tweets = scrape_twitter(args.keyword, args.count, args.headless, output_filename=final_filename)

